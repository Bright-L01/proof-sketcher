name: CI/CD Monitoring & Alerting

on:
  workflow_run:
    workflows: ["CI/CD Pipeline - Alpha Software"]
    types: [completed]
  schedule:
    # Daily health check at 9 AM UTC
    - cron: '0 9 * * *'
  workflow_dispatch:

permissions:
  actions: read
  issues: write
  contents: read

jobs:
  monitor-ci-health:
    name: Monitor CI/CD Health
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup monitoring environment
        run: |
          pip install PyGithub pandas matplotlib seaborn

      - name: Analyze CI/CD metrics
        uses: actions/github-script@v7
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;

            // Get workflow runs from last 7 days
            const since = new Date();
            since.setDate(since.getDate() - 7);

            const runs = await github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
              owner,
              repo,
              created: `>=${since.toISOString()}`
            });

            // Calculate metrics
            const metrics = {
              total_runs: runs.length,
              successful_runs: runs.filter(r => r.conclusion === 'success').length,
              failed_runs: runs.filter(r => r.conclusion === 'failure').length,
              cancelled_runs: runs.filter(r => r.conclusion === 'cancelled').length,
              avg_duration: runs.reduce((acc, r) => {
                if (r.run_started_at && r.updated_at) {
                  const duration = new Date(r.updated_at) - new Date(r.run_started_at);
                  return acc + duration;
                }
                return acc;
              }, 0) / runs.filter(r => r.run_started_at && r.updated_at).length / 1000 / 60 // minutes
            };

            metrics.success_rate = (metrics.successful_runs / metrics.total_runs * 100).toFixed(2);

            console.log('## CI/CD Metrics (Last 7 Days)');
            console.log(`Total Runs: ${metrics.total_runs}`);
            console.log(`Success Rate: ${metrics.success_rate}%`);
            console.log(`Failed Runs: ${metrics.failed_runs}`);
            console.log(`Average Duration: ${metrics.avg_duration.toFixed(2)} minutes`);

            // Store metrics for trends
            const fs = require('fs');
            fs.writeFileSync('ci_metrics.json', JSON.stringify(metrics, null, 2));

            return metrics;

      - name: Check quality trends
        run: |
          cat > check_quality_trends.py << 'EOF'
          import json
          import sys
          from datetime import datetime

          # Load current metrics (would normally load from database/storage)
          with open('ci_metrics.json') as f:
              metrics = json.load(f)

          # Alpha software thresholds (relaxed)
          THRESHOLDS = {
              'success_rate': 50,  # 50% success rate minimum
              'max_duration': 30,  # 30 minutes max
              'max_failures_per_week': 20  # Allow up to 20 failures
          }

          # Check thresholds
          alerts = []

          if float(metrics['success_rate']) < THRESHOLDS['success_rate']:
              alerts.append(f"⚠️ CI success rate ({metrics['success_rate']}%) below threshold ({THRESHOLDS['success_rate']}%)")

          if metrics['avg_duration'] > THRESHOLDS['max_duration']:
              alerts.append(f"⚠️ Average CI duration ({metrics['avg_duration']:.1f} min) exceeds threshold ({THRESHOLDS['max_duration']} min)")

          if metrics['failed_runs'] > THRESHOLDS['max_failures_per_week']:
              alerts.append(f"⚠️ Too many failures this week ({metrics['failed_runs']} > {THRESHOLDS['max_failures_per_week']})")

          # Generate report
          print("# CI/CD Health Report")
          print(f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
          print("\n## Summary")
          print(f"- Success Rate: {metrics['success_rate']}%")
          print(f"- Total Runs: {metrics['total_runs']}")
          print(f"- Failed Runs: {metrics['failed_runs']}")
          print(f"- Avg Duration: {metrics['avg_duration']:.1f} minutes")

          if alerts:
              print("\n## ⚠️ Alerts")
              for alert in alerts:
                  print(f"- {alert}")
              sys.exit(1)  # Fail if alerts
          else:
              print("\n## ✅ All metrics within acceptable ranges for alpha software")

          # Recommendations
          print("\n## Recommendations")
          print("- Focus on fixing the most frequent test failures")
          print("- Consider splitting long-running tests")
          print("- Implement test result caching")
          print("- Add more granular test categories")
          EOF

          python check_quality_trends.py

      - name: Create monitoring dashboard
        run: |
          cat > create_dashboard.py << 'EOF'
          import json
          import matplotlib.pyplot as plt
          import seaborn as sns
          from datetime import datetime

          # Load metrics
          with open('ci_metrics.json') as f:
              metrics = json.load(f)

          # Create dashboard
          fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
          fig.suptitle('Proof Sketcher CI/CD Dashboard', fontsize=16)

          # Success rate pie chart
          ax1.pie([metrics['successful_runs'], metrics['failed_runs'], metrics['cancelled_runs']],
                  labels=['Success', 'Failed', 'Cancelled'],
                  colors=['green', 'red', 'gray'],
                  autopct='%1.1f%%')
          ax1.set_title('Build Status Distribution')

          # Duration bar chart (placeholder data)
          durations = [15, 18, 12, 25, 20, 16, 19]  # Example data
          ax2.bar(range(len(durations)), durations)
          ax2.axhline(y=30, color='r', linestyle='--', label='Threshold')
          ax2.set_xlabel('Recent Builds')
          ax2.set_ylabel('Duration (minutes)')
          ax2.set_title('Build Duration Trend')
          ax2.legend()

          # Test coverage trend (static for alpha)
          coverage = [11] * 7  # Constant 11% for alpha
          ax3.plot(range(7), coverage, 'b-', marker='o')
          ax3.axhline(y=70, color='g', linestyle='--', label='Target')
          ax3.set_ylim(0, 100)
          ax3.set_xlabel('Days')
          ax3.set_ylabel('Coverage %')
          ax3.set_title('Test Coverage Trend')
          ax3.legend()

          # Quality metrics
          quality_scores = {
              'Tests': 11,
              'Security': 31,  # 100 - 69 vulnerabilities
              'Linting': 15,   # Very low due to 3625 issues
              'Docs': 60
          }
          ax4.barh(list(quality_scores.keys()), list(quality_scores.values()))
          ax4.set_xlim(0, 100)
          ax4.set_xlabel('Score %')
          ax4.set_title('Quality Metrics')

          plt.tight_layout()
          plt.savefig('ci_dashboard.png', dpi=150)
          print(f"Dashboard saved as ci_dashboard.png")
          EOF

          python create_dashboard.py || echo "Dashboard generation failed (expected in CI)"

      - name: Upload monitoring artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ci-monitoring-${{ github.run_number }}
          path: |
            ci_metrics.json
            ci_dashboard.png
          retention-days: 30

      - name: Send alerts if needed
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            // Create issue for CI health alert
            const title = `CI/CD Health Alert - ${new Date().toISOString().split('T')[0]}`;
            const body = `## CI/CD Health Alert

            The automated monitoring has detected issues with CI/CD pipeline health.

            ### Detected Issues
            Check the workflow run for detailed metrics and alerts.

            ### Alpha Software Note
            This is expected behavior for alpha software. These alerts help track improvement areas.

            ### Action Items
            - [ ] Review failed builds
            - [ ] Check for flaky tests
            - [ ] Consider infrastructure issues
            - [ ] Update test timeout values if needed

            Workflow Run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}
            `;

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['ci/cd', 'monitoring', 'alpha']
            });

  weekly-summary:
    name: Weekly CI/CD Summary
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
      - uses: actions/checkout@v4

      - name: Generate weekly report
        uses: actions/github-script@v7
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;

            // Get data from last week
            const since = new Date();
            since.setDate(since.getDate() - 7);

            const [runs, issues, prs] = await Promise.all([
              github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
                owner, repo,
                created: `>=${since.toISOString()}`
              }),
              github.paginate(github.rest.issues.listForRepo, {
                owner, repo,
                since: since.toISOString(),
                state: 'all'
              }),
              github.paginate(github.rest.pulls.list, {
                owner, repo,
                since: since.toISOString(),
                state: 'all'
              })
            ]);

            // Generate summary
            const summary = `# Weekly CI/CD Summary

            **Period**: ${since.toISOString().split('T')[0]} to ${new Date().toISOString().split('T')[0]}

            ## CI/CD Metrics
            - Total workflow runs: ${runs.length}
            - Success rate: ${(runs.filter(r => r.conclusion === 'success').length / runs.length * 100).toFixed(1)}%
            - Average duration: ${(runs.reduce((acc, r) => {
              if (r.run_started_at && r.updated_at) {
                return acc + (new Date(r.updated_at) - new Date(r.run_started_at));
              }
              return acc;
            }, 0) / runs.filter(r => r.run_started_at).length / 1000 / 60).toFixed(1)} minutes

            ## Development Activity
            - Issues opened: ${issues.filter(i => !i.pull_request && new Date(i.created_at) >= since).length}
            - Issues closed: ${issues.filter(i => !i.pull_request && i.closed_at && new Date(i.closed_at) >= since).length}
            - PRs opened: ${prs.filter(pr => new Date(pr.created_at) >= since).length}
            - PRs merged: ${prs.filter(pr => pr.merged_at && new Date(pr.merged_at) >= since).length}

            ## Alpha Software Progress
            - Current test coverage: 11% (target: 70%)
            - Known security issues: 69 (target: 0)
            - Code quality violations: 3,625 (target: <100)

            ## Recommendations
            1. Continue focusing on test coverage improvements
            2. Address critical security vulnerabilities
            3. Implement incremental code quality fixes
            4. Monitor CI performance trends
            `;

            console.log(summary);

            // Optionally create a discussion
            if (context.eventName === 'schedule') {
              // This would create a weekly discussion post
              console.log('Weekly summary generated (would post to discussions in production)');
            }
